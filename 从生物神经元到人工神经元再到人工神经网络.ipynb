{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生物神经元到人工神经元\n",
    "\n",
    "![](pic\\1.jpg)\n",
    "\n",
    "如上图，是我们生物学中最常见的一种神经元类型(神经元还是有不同种类的，种类不同样子也不一样)。\n",
    "\n",
    "这个神经元主要是由树突，细胞体和轴突构成\n",
    "\n",
    "这里仅仅简单的说一下这些部分都负责些什么？\n",
    " - 1. 树突，从名字上看就知道，看起来“枝叶繁茂”，这样的结构利于从其他很多个神经元处接受神经递质并转为电信号。\n",
    " - 2. 细胞体，这里是处理从树突得来的所有的电信号，如果信号值达到了动作电位，则会向轴突方向传递新的电信号。\n",
    " - 3. 轴突，因为其很长，是轴状，因此得名，其结构利于向其他很远的神经元进行连接，传递信息。\n",
    " \n",
    "简而言之，树突是接受从其他神经元传递来的信号，细胞体处理信号，轴突是向其他神经元发送信号。\n",
    "\n",
    "这里稍微提一下，一个神经元树突上的分支很多，因此可以接受很多来自其他神经元的信号，但是轴突只有一个，因此即使轴突与其他很多神经元相连，该细胞发送给他们的的电信号只能是相同的。\n",
    "\n",
    "# 人工神经元\n",
    "\n",
    "人工神经元相比生物神经元来说，抽象了不少。\n",
    "\n",
    "我们不再去管什么神经递质，化学信号转电信号。我们把神经元之间的信号统统指定为数值类型。这样，我们神经元之间信号的传递就可以理解为数值的运算。\n",
    "\n",
    "![](pic\\2.jpg)\n",
    "\n",
    "上图是抽象出来的人工神经元，左边的部分对应生物神经元的树突，中间的圆圈对应细胞体，右边则是对应轴突。\n",
    "\n",
    "其传递的原理也很简单，树突接受输入，经细胞体处理后，由轴突将新的信号传递出去。\n",
    "\n",
    "如果树突接受到的输入有n个，则我们记每一个输入为$x_i$，神经元的输出为$y$，神经元的信息传递可以写成：\n",
    "\n",
    "### $$y = \\sum_{i}^{n} x_i$$\n",
    "\n",
    "但是我们还需要加入两个个东西：神经元之间的连接强度和动作电位。\n",
    "\n",
    "有这样的假设，我们记忆或遗忘的过程，就是大脑中神经元之间创建或断开连接的过程，所谓连接，就是一个神经元的轴突到另一个神经元的树突。也许，神经元之间的连接强度越强，某件事情就记忆的越深刻。\n",
    "\n",
    "因此，我们在“树突”的每一条边上都增加一个属性：权重(Weight)，也就是我们的连接强度。\n",
    "\n",
    "至于动作电位这个东西，则相当于我们神经元接受到的输入信号的总和，减去动作电位后，再去作为输出。\n",
    "\n",
    "然后我们在细胞体上增加了一个属性，偏置(Bias), 也就代表着动作电位\n",
    "\n",
    "![](pic\\3.jpg)\n",
    "\n",
    "若记第$i$个输入对应的权重为$w_i$，神经元的偏置为$b$，则此时，我们单个神经元的传递公式变成了：\n",
    "\n",
    "### $$y = \\sum_{i}^{n} w_ix_i + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([2, 1, -2], np.float32)\n",
    "w = np.array([2, 3, 1], np.float32)\n",
    "\n",
    "b = - 1.0\n",
    "\n",
    "y = np.dot(w, x) + b\n",
    "print (y) # (4 + 3 - 2) - 1 = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人工神经元到人工神经网络\n",
    "\n",
    "上一期视频中提到了一种神经网络层，叫做稠密层，也叫全连接，这种人工神经网络层是最基础的一种网络层，他的结构很简单，如下图，可以看出其名称为什么叫全连接：\n",
    "\n",
    "![](pic\\4.jpg)\n",
    "\n",
    "红色的一排就是全连接层，其中的每一个红圈都是一个人工神经元，我们可以发现：全连接层中，相同层次的神经元之间没有连接，全连接层中的每个神经元与每一个输入都存在一个连接，当层中的神经元很多的时候，输入与神经元之间的连接十分密集，因此全连接层也叫稠密层。\n",
    "\n",
    "从图中也可以看出，全连接层的输入是一个向量，输出也是一个向量，输出的向量的维度就是层中神经元的个数。\n",
    "\n",
    "之前已经指出了单个神经元的传播公式，即输入向量与权重向量做点积后，再与偏置相加。全连接层中有多个神经元，如果我们将这些神经元并排拍到一起，他们的权重也分别排在一起，则偏置和输出都变成了一个向量，权重变成了一个矩阵。\n",
    "\n",
    "传播公式也相应的变成了：\n",
    "\n",
    "### $$y = Wx + b$$\n",
    "\n",
    "其中x是输入向量，W是全连接层中的每个神经元的权重向量组成的矩阵，b是每个神经元的偏置组成的向量，y就是全连接层的输出向量了。\n",
    "\n",
    "可以看到，全连接层其实就是对我们的输入空间做了一次放射变换，即空间上的缩放，旋转，切变和平移。但这些变换本质上都是线性的，这在深度学习中并不是一个很好的性质，这部分会在下个有关于线性非线性、激活函数的视频中讲解到。\n",
    "\n",
    "全连接层是神经网络层中最基础的结构，其在大部分任务上都有着还不错的表现，但也正是如此，在一些特定的任务上，单使用全连接的表现并没有一些专门针对这些任务的网络表现的好(也就是会的多，但不精通)，比如图像任务上有着专门的卷积层，序列任务上有了专门的循环神经网络层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "\n",
    "W = np.ones([3, 4]) # 创建一个shape = (3, 4)的全1填充的数组\n",
    "b = np.ones([4])\n",
    "\n",
    "y = np.matmul(x, W) + b\n",
    "print(y)\n",
    "\n",
    "# 输出的每一个元素都等于: （1 * 1 + 2 * 1 + 3 * 1） + 1 = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然已经知道了全连接神经网络的原理，那么我们这次会尝试一下，使用我们得到的传播公式去替代之前我们使用keras搭建的全连接层。\n",
    "\n",
    "首先，先把上一节课用到的'input_data.py'和训练数据文件夹'mnist_data'放到与我们jupyter笔记相同路径之下。\n",
    "\n",
    "导入tensorflow等文件，加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# 读入数据\n",
    "mnist_data = input_data.read_data_sets('mnist_data', one_hot=True)\n",
    "\n",
    "train_images = mnist_data.train.images\n",
    "train_labels = mnist_data.train.labels\n",
    "\n",
    "plt.imshow(np.reshape(train_images[5], [28, 28]), cmap='gray')\n",
    "plt.show()\n",
    "print ('        ', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "print ('OneHot: ', train_labels[5])\n",
    "\n",
    "\n",
    "test_images = mnist_data.test.images\n",
    "test_labels = mnist_data.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上节课提到了OneHot编码，但是没有讲这个编码是什么意思。\n",
    "\n",
    "所谓OneHot编码，翻译过来是独热编码，换句话说就是只有一个元素个是1，其他全是0.\n",
    "\n",
    "这里我们编码8，得到的是一个长度为10的0~1数组，10是因为我们图片只有10个种类，然后数组的第9个元素(也就是arr[8])等于1，因为这张图片写的就是8。\n",
    "\n",
    "若我们要编码一个数字x，x的取值情况有$[0, n]$ n种，则得到的OneHot编码 arr 的长度就是n，且有$arr[x] = 1$，其余元素都等于0。\n",
    "\n",
    "至于为甚么要编码，首先OneHot编码出来的也是一个概率分布，而我们的神经网络的输出值的也是长度为10的概率分布，这样就可以方便的度量这两个概率分布的差距，为我们之后的训练做铺垫。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # 因为训练全连接层，就相当于找到一个合适的权重和偏置，因此申请两个可训练的变量，作为层的权重和偏置。\n",
    "        self.W = self.add_weight(shape=[input_shape[1], self.output_dim], initializer=tf.random_normal_initializer(), trainable=True)\n",
    "        self.b = self.add_weight(shape=[self.output_dim], initializer=tf.ones_initializer(), trainable=True)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):\n",
    "        res = tf.matmul(input, self.W) + self.b\n",
    "        return res\n",
    "\n",
    "input_ = tf.keras.Input(shape=(784, ))\n",
    "\n",
    "Layer_dense1 = MyDense(128)\n",
    "dense_1 = Layer_dense1(input_)\n",
    "# MyDense(128) 即MyDense类的构造函数，128对应output_dim参数\n",
    "# Layer_dense1(input_)会调用类种的build函数，传递进来的参数是输入的shape，此时可以根据输入输出的尺寸定义需要训练的变量\n",
    "activate_1 = tf.nn.tanh(dense_1)\n",
    "\n",
    "dense_2 = MyDense(10)(activate_1)\n",
    "out = tf.nn.softmax(dense_2)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_, outputs=out)\n",
    "model.summary()\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=train_images, y=train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    pred = model(tf.expand_dims(test_images[i], axis=0))\n",
    "    img = np.reshape(test_images[i], (28, 28))\n",
    "    lab = test_labels[i]\n",
    "    print('真实标签: ', lab, '， 网络预测: ', np.argmax(pred.numpy()))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow中可以被训练的东西：\n",
    "\n",
    "TensorFlow中的Tensor可以分为几个类型：\n",
    " - 1.变量，若属性trainable为真，则可训练\n",
    " - 2.常量，创建时设置初值，不可被训练，也不可被改变\n",
    " - 3.占位符，一般作为网络的输入，平时不设定值，只有网络传播时才指定其值，输入一般不会进行训练，但不是不可训练(例子，图像风格迁移)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python36864bittf2condaa3c1267118644d0dab19695901f139a7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
